{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a Project of a Chatbot based on LLM\n",
    "\n",
    "This is a chatbot example using Transmilenio as the main topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirements\n",
    "\n",
    "As follows the required dependencies will be instaled into workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers faiss-cpu datasets fastapi uvicorn pymupdf accelerate nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract PDF Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "path = \"./informacion_general.pdf\"\n",
    "text = extract_text_from_pdf(path)\n",
    "print(text)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate Model based on a Foundational Model and Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anunezb/.pyenv/versions/3.12.4/envs/python_env_3.12.4/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.75 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.7817802429199219, metrics={'train_runtime': 14.3268, 'train_samples_per_second': 0.209, 'train_steps_per_second': 0.209, 'total_flos': 397402195968.0, 'train_loss': 0.7817802429199219, 'epoch': 3.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# load foundational model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# prepare specific data\n",
    "path_concepts = \"./informacion_general.pdf\"\n",
    "text_concepts = extract_text_from_pdf(path_concepts)\n",
    "\n",
    "# Add a dummy label for the purpose of training\n",
    "# In a real scenario, you should have actual labels for your data\n",
    "dataset = Dataset.from_dict({\"text\": [text_concepts], \"labels\": [0]})\n",
    "\n",
    "# tokenize data\n",
    "def tokenize_function(concepts):\n",
    "    return tokenizer(concepts[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# train model with fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModel, AutoTokenizer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Embed definition\n",
    "def embed(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "embedding_model_name = \"distilbert-base-uncased\"\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "embedding_dim = embedding_model.config.hidden_size\n",
    "\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "documents = [\"./technical_report.pdf\"]\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    import fitz  # PyMuPDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "embeddings = np.vstack([embed(extract_text_from_pdf(doc), embedding_model, embedding_tokenizer) for doc in documents])\n",
    "index.add(embeddings)\n",
    "\n",
    "def retrieve(query: str) -> list:\n",
    "    query_embedding = embed(query, embedding_model, embedding_tokenizer)\n",
    "    query_embedding = np.array(query_embedding).reshape(1, -1)  \n",
    "    D, I = index.search(query_embedding, 1)\n",
    "    return [documents[i] for i in I[0]]\n",
    "\n",
    "def generate_response(prompt: str) -> str:\n",
    "    retrieved_document = retrieve(prompt)\n",
    "    context = \" \".join(retrieved_document)\n",
    "    input_text = f\"{context} {prompt}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    output = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],  # Give more attention to non-padding tokens\n",
    "        max_length=150, \n",
    "        num_return_sequences=1, \n",
    "        temperature=0.7,  # Control randomness\n",
    "        top_k=50,         # Controla diversity\n",
    "        top_p=0.9,        # Controla diversity\n",
    "        repetition_penalty=2.0,  # Penalize repeated output\n",
    "        pad_token_id=tokenizer.eos_token_id  # Configure the pad token\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example of use\n",
    "prompt = \"What do you know about bogota?\"\n",
    "response = generate_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deploy FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Query(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "@app.post(\"/chatbot\")\n",
    "def chatbot(query: Query):\n",
    "    try:\n",
    "        response = generate_response(query.prompt)\n",
    "        return {\"response\": response}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env_3.12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
